\section{Introductionnn}

In recent years, several reports surfaced, showing that extensive attempts at replicating experiments and reproducing results from peer-reviewed research fails in a significant proportion of cases across many disciplines~\cite{ionnadis:2005,anderson:2017}. Especially studies pertaining to the life sciences have been widely reported in the media, contributing to the equally widespread notion that science in general faces a so-called replication crisis,%
\footnote{\scriptsize\url{https://en.wikipedia.org/wiki/Replication_crisis}}
or even to the widespread mistrust in science these days. Notwithstanding the fact that such a generic claim about the lack of reproducibility of science across disciplines cannot hold, computer science is not exempt from this problem. Unlike most other disciplines, however, its data/software-driven sub-disciplines have the potential to oust irreproducible research almost entirely. The remedy is simple: publish all data and code alongside your paper.

For all its simplicity, computer scientists are still not accustomed to publishing data and code~\cite{collberg:2015}, and the reasons for that are aplenty~\cite{stodden:2010}: lack of time to polish these resources, lack of time to support others in using them, lack of incentives since data/software publications ``do not count,'' fear of third parties identifying mistakes that lead to an invalidation of one's contributions, fear of handing third parties a shortcut to competing with oneself, fear of missing out on potential commercial opportunities, and legal problems pertaining to copyrights and proprietary data/software, to name only a few.
Opposed to that, and strongly in favor of publishing data/software, are things like a higher potential for others to build on and reuse one's approaches resulting in more citations, a faster pace of progress in a discipline, long-term preservation beyond the date a PhD student moves on, a more direct reference to understand an approach besides its abstract description in a paper, and an implementation of an approach that has been optimized to its author's liking rather than anybody else's not involved with its original development.

Apart from leading by example, whenever one wishes to employ a third party's approach that is neither available up front, nor on request, the only solution left is to reimplement it, tracing the steps outlined in the paper describing an approach. As many a computer scientist can tell from sad experience, using a paper as a kind of recipe for reimplementation will often result in many questions remaining unanswered~\cite{fokkens:2013}---gaps that, in the absence of feedback from a paper's authors, must be filled based on one's own idea of what the author might have intended, or based on what is commonly done in similar situations. Reimplementing the paper may often feel like reinventing the approach again. Reimplementation may in fact be considered the ultimate test of whether a paper ``works'' as intended. In this regard, satisfying a reviewer is not quite the same as satisfying a reimplementer. At any rate, reimplementing an approach solely based on its description is a valuable exercise, both in terms of getting to grips with a field of research, as well as learning from first-hand experience what needs to be in a paper for it to be easily reimplementable. Furthermore, it tells something about the robustness of an approach if its general behavioral characteristics can be reproduced even under incomplete information. That said, every paper should still contain sufficient information for precise replication of its experiments.

In this paper, we contribute a large-scale study on the reproducibility of a selection of important author identification papers. A total of 15~approaches are reimplemented, and the resulting code bases shared for the benefit of the wider author identification community. We further report on issues identified with the papers that sometimes prevented an accurate replication, and the first comparative evaluation involving all reimplemented approaches. We contribute to the growing repository of open source reimplementations of author identification approaches, placing this sub-dicipline of natural language processing one step closer to a complete reimplementation of its legacy.
