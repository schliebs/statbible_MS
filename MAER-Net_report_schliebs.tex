\documentclass[a4paper,man,natbib]{apa6}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{epigraph}
\usepackage{ragged2e}


\title{MAER-NET Conference Friedrichshafen Oct 2017}
\shorttitle{MAER-NET}
\affiliation{Marcel Schliebs}
\author{Executive Summary of talks}



\begin{document}
	
\justifying
\maketitle

\tableofcontents

\justifying

\section{Introduction}

During the last couple of years, meta-analysis has continually gained wider acceptance not only within the fields of medicine or biology, but also within economic and social sciences, yielding a powerful tool in order to for example evaluate practical implications of public policy. \\

From October 12 to 14 2017, Zeppelin University had the change to host the 10th edition of the MAER-Net conference, providing participating researchers as well as academically interested students a unique platform for exchange and the collaborative exploration of new areas within meta-analysis.\\

During three intensive days of academic presentations and interpersonal exchanges, the MAER-Net Colloquium enabled interdisciplinary researchers to increase their knowledge and discuss ideas with about 60 colleagues and approximately 40 presentations on topics from monetary economics over education and fiscal policy up to methodological advancements.

Key speakers included \textbf{Edward Miguel}, the Oxfam Professor of Environmental and Resource Economics in the Department of Economics at University of California, Berkeley, and director of the international initiative for greater transparency among the social sciences—Berkeley Institute for Transparency in the Social Sciences (BITSS), who presented the concept of \textit{Pre-Analysis-Plans} and their potential applicability to Meta-Analysis as well as 
\textbf{Tom Stanley},the convener of MAER-Net who has been developing meta-regression methods for more than 30 years, who talked about current challenges of meta-analysis and possible solution approaches.

\newpage
\justifying
\section{Fidrmuc \& Kapounek: Bayesian model averaging in meta-analysis}
\label{sec:BMA}

Fidrmuc and Kapounek presented a Bayesian model averaging approach to meta-analysis. Based on the principle theorem 
$P(\theta|\textbf{y}) = P(\theta)\frac{P(\textbf{y}) |\theta)}{P(\textbf{y})}$ 
,whereby the noninvolvedness of $\theta$  within $P(y)$ implies
$P(\theta|\textbf{y}) \propto P(\theta )P(\textbf{y} |\theta)$ (cf. \cite{Gelman2014}), 
they developed the applicability of BMA as an answer to common challenges that arise with the age of Big Data. With classical frequentist statistical tools historically having been custom-tailored to observational studies and experiments with large numbers of observations, the modern macroeconomics and meta-analysis in general share the challenge of so called \textit{fat data}(cf. \cite{Varian2014a}), namely a large number of (potentially) explanatory variables but a comparatively relatively low observational dimension. In such a context, Bayesian models relying on informative priors\footnote[1]{Attention: Depending on the data density, the results may be very sensitive to such priors} can perform significantly better than their frequentist OLS counterparts.\\
Fidrmuc \& Kapounek thus applied two forms of Bayesian sets of methodology, with the first being \textit{Bayesian Model Averaging (BMA)}, namely calculating the weighted average of estimates weighted by the respective posterior probability density $p(M_r|\textbf{y})$, before they conduct \textit{Bayesian Model Selection (BMS)}, for the choice of a single model with respective estimates.
Through the application of cross-sectional regression, this procedure leads to a large number of $R = 2^k$ different possible subsets of the model matrix. This analytical challenge is solved by a \textit{$MC^3$ Monte Carlo Markov technique} algorithm with $2*10^6$ iterations, of which about the first 10\% are discarded as burn-ins. In an analysis of the effects of the \textit{Basel III legislation} on GDP, Fidrmuc \& Lind (cf. Panel 3a) first use BMA to calculate posterior model densities before selecting a single model through BMS.\footnote[2]{Remaining explanatory covariates are: Banking Sector, Other Origin, Bank based System, VAR models, macro-structural models, CES models as well as monetary policy offset} \\Drawing conclusions from Fidrmuc \& Kapounek's methodological proposal, the complementary application of BMA and BMS offers an innovative approach and a Bayesian response to the challenges presented by Fat Data, at the same time reducing the risk of overfitting to statistical noise and creating more precise estimates. 

\justifying
\section{Stanley: Recent developments - Should we WAAP economics’ low power and exaggerated results?}
\label{sec:WAAP}
Prof. Stanley provided a summary of the short but intensive history of 10 years that MAER-net has been growing for, both emphasizing the institutional and interpersonal development of the network as well as the scientific findings that the cooperative research has produced, among which \textit{Publication Bias}, \textit{Low Power} and \textit{High Heterogeneity} stand out as the most prominents. As for Publication Bias, Stanley, with referrence to his introductory keynote stressed out that funnel graphs are often relatively skewed, which according to his interpretation should arise most likely due to selective reporting of statistically significant findings in the "right" direction. Technically, as Stanley warned the audience however, one can never doubtlessly infer publication bias on a micro- or meso-level but rather evaluate socio-economic theories about individual researcher-behavior. Building on a sample of 159 meta-analyses including 6700 papers with 64.076 estimates, Stanly together with Ioannidis and others \cite{Ioannidis2016} found effect sizes in economics to be typically inflated by as much as 100(!)\%, thus confirming the proposed \textit{Paldam Principle}. \\
Referring to the frequently published \textit{Economic Journal} paper (\cite{Stanley2017}), he hence went on to elaborate on the contemporary shortcomings within quantitative research with regards to \textbf{Low Statistical Power}. After defining Statistical Power as $1-\beta$ with $\beta$ being the probability of a type II error\footnote[3]{A type II error is the mistake of accepting that there is no effect when, in fact, there is a genuine effect.}, Stanley claimed that in fact \textit{statistical power, not p-values were the real metric of importance of an empirical result}.\\
After finding that a large majority of studies contained within in the sample of 159 meta-analyses were underpowered, Stanley proposed the measure of \textbf{WLS Weighted Average of only the Adequately Powered (WAAP)}, which is able to provide a conservative assessment of bias, that is to say an empirical lower bound, thus consistantly reducing bias (although not completely eliminating it) and improving performance in few-studies-cases, in which PET-PEESE typically performs inferiorly. As for practical implications, Stanley proposed that all meta-analyses shall \textbf{report median power} along with estimates and model checks. As final advice for scientists conducting meta-analyses, Stanley emphasized five principles: Being modest about results, caution in case of pub’bias, high heterogeneity and evidence of small effects and emphasizing practical significance. Furthermore, one should interpret all simple MAs and MRAs as descriptive or indicative, not definitive and last but not least routinely conducting multiple MRA with many moderator variables and always reporting $I^2$ and median power. 




\section{Plenary 2: Methods of Meta-Analysis}
\label{methodology}
\justifying

\subsection{Bernd Weiss: Heteroskedastic mixed-effects meta-regression}

Bernd Weiss, responsible für developing open science strategy for department survey design and methodology at \textit{GESIS}, presented a procedure to estimate ANOVA-like mix effects models including two error terms, with the first capturing the difference between observed effect sizes and study-specific simulations and the latter referring to the average of all studies. \\
Within this framework, the \textit{random effects model} is described by
\begin{equation}
	t(i) = \mu + u(i) + \epsilon(i)\ with \epsilon ~ N(0,\sigma^2)
\end{equation}

Under the random effects model, variation arises from two sources, namely sampling error and true differences in effects. The variation among groups can be modeled by matrix notation: 
\begin{equation}
T = X * \beta + u + \epsilon \ with \left\{
\begin{array}{ll}
T = kx1 vector with the observed effects\\
T ~ N(X*\beta,\psi+V)
\end{array}
\right.
\end{equation}
The goal is thus to examine the overall effect and, more interestingly, \textit{heterogeneity between models} (first considered by \cite{Hedges1982}). 
Weiss hence presented three approaches to analyzing between-studies variability, namely (1) theory-based, (2) using a graphical representation Cis-comparing Q-profile method and (3) a Likelihood-Ratio-Test. 
Using example data from \cite{Ehri2001}, Weiss focused on the moderator analysis for socioeconomic status and found significant heterogeneity to be extracable among the thirty effect sizes, which suggests the presence of between-studies variability. 
Calculating four possible between-studies variances using MLE from the \textit{metafor} R-package, Weiss examined the respective groups for their between-studies variances. However, an important limitation remains in situations in which subgroups will not have enough effect sozes to estimate between-studies  variances accurately and consequently using LRT will be unwise in thos situations. 


\subsection{Mohammad Jalali: A flexible method for aggregation of prior statistical findings}

Mohammad Jalali from MIT presented a methodological approach for aggregating prior findings relying on heterogeneous statistical models. As one example among many others within science, he presented a set of different studies proposing a variety of heterogeneous models predicting Basal Metabolic Rate.\footnote[4]{with common explanatory variables being weight, lean mass, BMI, fat mass, age and height.} 
However, as other contributions during the conference similarily reminded, meta-analysis can be tricky when multiple independent variables with varying items, different variable definitions/transformations as well as diverse model structures  come into play. The approach developed by the MIT-researchers assumes an underlying true (but unkown) model 

\begin{equation}
y = f(x,\epsilon,\beta_0), with \left\{
\begin{array}{ll}
x = [x_i], i = 1,2,...,I\\
\beta_0 = [\beta_0j], j = 1,2,...,J\\
E(\epsilon) = 0
\end{array}
\right.
\end{equation}
which is computationally simulated within a \textit{Generalized Model Aggregation (GMA)} framework relying on the inputs of information of empirical signatures from prior studies (main numerical input), proposed meta-model structures (qualitative information) as well as information on independent variables such as means or (co-)variances detained from other data sources (auxiliary numerical input). This framework hence allows it to \textbf{estimate a main beta-model by simulating prior studies and minimizing the weighted difference between simulated and empirical differences.} Applied to the introduced example of estimating a model for Basal Metabolic Rate, the proposed GMA procedures resulted in a significant reduction of mean absolute percentage error as compared with other studies and common baseline models. The GMA approach thus provides a flexible method for aggregating diverse research findings into a meta model and hereby introduces a potential new way for estimating mechanism-based models solely on the basis the reported results of previous studies (without having access to primary data), a procedure that promises diverse application opportunities across economics, biology, medicine and other subfields of science.\footnote[5]{For further model specification, see \cite{Rahmandad2017}}

\subsection{Stephan Bruns: Star wars: power needs to awake}

In his talk, Bruns started with the conclusion that reproducibility is at the top of the agenda across the sciences (\cite{Collaboration2015};\cite{Camerer2016};\cite{Wasserstein2016}), with recent and prominent failures to reproduce experiments having caused the so-called \textit{replication crisis} and a recent debate on (the future of) \textit{p-values}. These topics lead Bruns to question and elaborate on the issues of both \textbf{p-value-hacking} (\cite{Simonsohn2014a}) as well as \textbf{publication bias} (\cite{Rosenthal1979}) as two of the main explanatory drivers behing empirical anormalities. In the presented piece of research, the authors estimate the extent of inflated p-values (or to be terminologically correct: \textit{z-values}) in economics using a counterfactual distribution of p-values or their respective z-values\footnote[6]{what would be in fact t-values are assumed to be z-values due to insufficient information regarding degrees of freedom}, with the z-values being calculated as: 
\begin{equation}
z_ij = \frac{\beta_ij}{se_ij}, with \left\{
\begin{array}{ll}
i \ being\ an\ index\ for\ observations\\
j\ being\ an\ index\ for\ research\ fields\\
\beta_ij\ denoting\ the\ estimated\ elasticity\\
and\ se_ij\ standing\ for\ the\ respective\ standard\ error.\\
\end{array}
\right.
\end{equation}
This counterfactual distribution is hereby estimated to be normally distributed with the counterfactual z-score being $z_ij ~ N(\frac{\beta_ij}{se_ij},1)$. Since uncertainty about the \textit{exact} counterfactual z-values remains, the authors used a simple Monte-Carlo simulation to obtain the 0.025 and 0.975 quantiles of the counterfactual distribution of published p-values.\\
Comparing the counterfactually derived distribution with its empirical counterpart, the authors come to the conclusion that 28.84\% of all p-values are inflated, while this is the case for as much as 45,80\% of \textit{significant} p-values. These results present an astonishing indication that there is reason to belief that estimates are indeed biased, most probably attributable to selective reporting with the goal of increasing the probability of achieving a successful publication. 

\subsection{Pedro Bom: A kinked meta-regression model for publication bias correction}
Bom introduced the concept of \textit{EK}, a 2-step endogenous linear meta-regression model relying on the\textit{PET-PEESE}-Estimate as an input. He hence compared the means as well as mean sqared errors for different selection incidences ($\pi\%$) and over different measures, namely \textit{FE, PET, PEESE, PET-PEESE, Top10 and EK}. The simulations were further differentiated in a moderate true effect of $\beta_0 == 1$ and a small true effect of $\beta_0 = 0.2$.\\
Derived from the empirical findings, Bom drew the conclusion that EK is generally less biased and more efficient than alternative emthods, especially in the case of severe publication selection. Further, EK was shown to be as a flexible solution to the \textit{true-effect dependence}-problem, while also tackling heterogeneity relatively well, especially for high incidence of publication selection.\\
\section{Panel 1b: Monetary Economics}

\subsection{Iikka Korhonen: Business cycle synchronisation in a currency union: taking stock of the European evidence}

Iikka Korhonen, Head of Research at the Bank of Finland, treated the question of what direction a post-Brexit Europe could steer to, namely a two-speed construction versus an ever closer political, economic and financial Union. He therefore identified the need to better grasp both core and periphery dynamics, since in the framework of a monetary union, it is of key importance to identify how correlated the business cycles are. This is of practical relevance since a higher degree of business cycle synchronisation implies smaller costs of currency sharing. 
Within two graphical mapplots, Korhonen highlighted a higher business cycle synchronisation through the introduction of the Euro, that also transcended to other peripheral countries. In the presented paper, Korhonen tries to answer the question whether it was currency directly that lead to an increase in cycle synchronisation or rather whether latent third variables, such as the single market, played a major role. \\
With the use of the meta-regression model 
\begin{equation}
1/2 log (\frac{1+p}{1-p} = \hat{p} + \sum_{k}^{k} \beta D + gr = 1)) with \left\{
\begin{array}{ll}
	p\ being\ average\ correlation\ coefficient\\
	for\ country\ i\ \\
	controlling\ for\ K\ factors\ (such\ as: \footnote[7]{publication year, variable, methodology, sample size, frequency, author affiliation, journal or not})\\
	in\ publication\ j
\end{array}
\right.
\end{equation}

based on a manually collected\footnote[8]{using google scholar, ssrn and repec} dataset, inclucing 63 papers with 2979 estimates, Korhonen showed that business cycle synchronization significantly increased after the introduction of the single increase. While this was more heavily the case for Euro members, the effect was also measurable for peripheral EU- and non-EU countries. The meta-regression was conducted by stepwise ommission of insignificant variables, whereby a the additional application of a Bayesian model averaging approach yielded almost identical results. 
Methodologically, the study thus offers a perfect example of a case in which a small positive effect cannot be statistically significantly detected in any single study but becomes detectable through the use of meta-analysis. 


\subsection{Andrej Cupak: Comparison of forecast performance of DSGE and VAR models: a meta Analysis}

Cupak from the National Bank of Slovakia presented a comparison between the forecast performances of DSGE versus VAR models using meta-regression. With DSGE models having emerged to be relatively popular modeling tools \cite{Linde2016}, their success record in forecasting performance of e.g. GDP growth have contained \textit{"hits and misses"} over the course of previous years. Cupak recognizes this as a motivation for developing a comparison framework of DSGE models on the basis of 700 estimates from 30 papers with the meta dependent variable being 
$\psi  = \frac{RMSE\ DSGE}{RMSE\ Benchmark}$,thus the ratio oft he RMSE of a given economic \\variable (gdp growth or inflation rate) obtained from DSGE model and the RMSE from \\a bayesian VAR benchmark), leading to the meta-regression model 

\begin{equation}
\psi= \alpha + \beta * M_j + \epsilon_j with \left\{
\begin{array}{ll}
\psi\ being\ the\ dependent\ variable\\
M_j\ representing\ the\ vector\ of\ explanatory\ variables\ \\
and\ \epsilon_j\ being\ the\ error\ term \\
\end{array}
\right.
\end{equation}
The empirical analysis yielded the result that generally, DSGE models produce more accurate forecasting results in 65\% of cases, wherby the higher impact factors tend to increase the model performance.\\
Further ideas for model improvement presented by Cupák include updating the sample, adding possible new studies, inspecting other determinants as well as including nominal short term interest rates in addition to gdp growth only.


\subsection{Martina Jancokova: Financial globalisation, monetary policy spillovers and	macro-modelling: tales from 1001 shocks}

Jancokova developed her work around the finding of a dramatic rise of financial globalisation since 1990 in combination with the parellel evolution of structural macro-modelling. Deriving the research question \textit{"Do standard New Keynesian DSGE Models fail to account for powerful financial spillover channels?} from this specific constellation, the approach presented by the ECB-researcher provides an answer to the opaque question in which ways the prominent emergence of financial globalisation and spillovers over the course of past decades leading to powerful cross-border financial spillover channels influence and shape structural monetary models. Against this back- ground, the paper proposes the hypothesis that New Keynesian DSGE models that lacking the inclusion of spillover channels may confound the effects of both national as well as foreign disturbances, if they are confronted with data. \\
Jancokova hence went on from this hypothesis to derive predictions and subject them to data on monetary policy shock estimates for 29 economies obtained from more than 280 monetary models in the literature(c.f. \cite{Georgiadis2017}). Consistent with the predictions from the developed hypotheses, the results of repetetive Monte Carlo simulations operationalizing spillover estimates as 
\begin{equation}
y_{us,t+h}  = \alpha^h + \sum_{k = 0}^{p}\gamma_k^h*\hat{e}_{ea,t-k})^{mp} + \sum_{k = 1}^{n}\delta_k^h*y_{us,t-k} +
\sum_{k = 0}^{q} x_{us,t-k}*\beta_{k}^{h} + u_{us,t}^{h}
\end{equation}
showed that monetary policy shock estimates derived from New Keynesian DSGE models that do not sufficiently account for financial spillover channels are contaminated by what she described as a "common global-component";with levels of contamination being significantly more severe for economies classified as a-priori more susceptible to financial spillovers channels. As Jancokova mentioned in her outlook, none of these findings to similarily apply to monetary policy shock estimates obtained from other models such as VAR, financial market expectations or the narrative approach.


\section{Panel 3a: Public Policy}
\subsection{Ronja Lind: Macroeconomic impact of Basel III: Evidence from a metaanalysis}

R. Lind presented a meta-analysis approach to examining the Macroeconomic impact of Basel III on GDP. In light of the 2008 financial crisis, global regulatory actors had agreed on a substantial increase in regulatory requirements and tools, while their assessment from the perspective of a potential macroeconomic \textit{trade-off} is a rather new approach. Yet, there is still high uncertainty as to how banks might respond to future increses in macroprudential regulation. \\
From a scientific perspective, the problem arises from the structure of contemporary approaches, with extremely heterogeneous models and modeling assumptions making direct comparison impossible. Hence, meta-analysis offers a innovative approach relatively new in the field of finance. 
The main contribution of Linds approach is based on a unique dataset of 48 studies including more than 300 estimates on the Basel III reforms. Conducting the meta-analysis, Lind finds the median estimate for the regulatory effect to be of negative albeit moderate size, with a surprisingly low level of heterogeneity between countries and their respective financial systems. \\
Further illustrating the findings in the form of histograms and funnel graphs, Lind showed the distribution of the GDP effect to be negatively skewed, with the funnel graph suggesting the potential presence of publication bias, a presumption that was hence further strengthened by a FAT-Test. \\
The meta-analysis was finally conducted accounting for variances in public characteristics, author affiliations, regional focus as well as model classes and definitions, with Bayesian Model Averaging being used for robustness analysis. Based on her empirical findings, Lind concluded that there seems to be a moderate negative effect (with the skewness suggesting potential publication bias), although empirical results must yet be interpreted with some caution.

\subsection{Marco Mariani: Evaluating public supports to the investment activities of business firms: A meta-regression analysis of Italian studies}

Mariani presented a hierarchical approach, which, with a focus on the Italian literature, introduced a model that does not only include dependence within-in-study estimates, but also extends the potential interdependencies to the cross-study-dimension. Using a hierarchical dataset of 50 studies since 2000 reporting over 1000 causal effects, Mariani responded to large heterogeneity in model specification and measurement by creating a discrete response variable for both the sign and the statistical significance level of the treatment effect estimate. This resulted in a binary outcome function of the $t-statistic$ with the explanatory variables being class K or Z variables (K = influence selection; z = government level, z = methodology). He then went on to estimate a multilevel MR (random intercept) model by the application of maximum-likelihood, within which the random effect is rewritten as a dependence effect: $v = \rho * Wv + u\ with \rho being\ the\ autoregressive\ coefficient\ that\ qualifies\ strength$\\
Marianis research design thus offers an innovative approach to imposing time-structure in order to better treat the commonly assumed presumption that independence existed between studies, not accounting for correlation if frequent scholars frequently co-author or copy theoretical and methodological approaches from each other.


\subsection{Fabienne Lind: The knowledge gap hypothesis: A meta-analysis}

F. Lind conducted a meta-analysis on the knowledge gap hypothesis, which treats the question in which way the access and use of mass media mediates the effect of education on political knowledge. Historically, different theories, reaching from a positive to a negative interaction effect, have been empirically presented, often with significantly diverging model assumptions and operationalization approaches.\\
The paper presented by Lind thus has the objective to not only summarize prior findings of the knowledge gap, but also clarify doubts about the validity of the knowledge gap hypothesis itself and further examine the impact of diverse moderating variables recently introduced into the scientific discussion. On the basis of 54 prior studies, Lind examined different types of relationships, namely the \textit{pure} education-knowledge relationship as well as a moderating effect of time lag on education knowledge and a moderating effect of media issue salience. 
After the data preparation in form of the conversion of coefficients to the effect size metric R, Lind on the basis of descriptive and graphical representations found support of the basic assumption of a positive education-knowledge relationship. For future evolutions of her research, she further proposed to adress the impact of mass media information with ideally more than the three preliminary ways of measurements and to shift the focus also to studies dealing with the effects of online information. 

\newpage
\section{Personal Reflection and Conclusion}

For me as a young researcher, the participation at MAER-Net offered a unique opportunity to discover new methodological approaches to many of the scientific challenges that have swirred around my head ever since I first got in touch with research, which has since become not only my career goal but also passion.
\newline
Often frustrated with individual systemic incentives (publication bias, p-value hacking, intransparent data) leading to yet-unsolved systematic inefficiencies of the academic system, I continually look out to ways and methods which could help me to later be a part of an even better world of science. Within this process, making own, even if small, contributions, fullfills me with great satisfaction, which is why I have started to replicate \textit{single} studies or papers in my free-time, so far focussing on detecting individual short-comings in the fields of case-selection, model specification, data (re-)coding and management. 
\newline
Having got in touch with the technique of meta-regression, I have immediately sensed the feeling of having discovered a new (and potentially pathbreaking) tool in my toolset of becoming a scientist that might one day be able to contribute to what we might idealistically call \textit{a better system.}\\
Within the set of presented models, I have been particularly thrilled to see the growing influence of Bayesian methods, of which I would describe myself as a great supporter and follower, although my application of them lays in (electoral/political) forecasting rather than economic indicators up-to-now. 
\newline
Especially against the backdrop of a (scientific) world in which we have access to ever more papers, data, models and literature (short: information), I leave this inspiring weekend with the conviction that meta-analysis will shape how science is conducted for many years. With regards to the (up to know, not yet fully intruded) field of political science, I hope to maybe be young pioneer myself and advance the study field by applying meta-regression-analysis to research question relevant to the area of political science. 


\nocite{*}
\bibliography{C:/Users/Schliebs/OneDrive/latex/bibtex/MAER}
\end{document}

%
% Please see the package documentation for more information
% on the APA6 document class:
%
% http://www.ctan.org/pkg/apa6
%

